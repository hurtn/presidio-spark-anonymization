{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**First run these uitility functions...**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\r\n",
        "\r\n",
        "from presidio_analyzer import EntityRecognizer, RecognizerResult\r\n",
        "from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\r\n",
        "\r\n",
        "class SpacyMagic(object):\r\n",
        "    \"\"\"\r\n",
        "    Simple Spacy Magic to minimize loading time.\r\n",
        "    >>> SpacyMagic.get(\"en\")\r\n",
        "    <spacy.en.English ...\r\n",
        "    \"\"\"\r\n",
        "    _spacys = {}\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def get(cls, lang):\r\n",
        "        if lang not in cls._spacys:\r\n",
        "            import spacy\r\n",
        "\r\n",
        "            from typing import List, Optional, Dict, Any\r\n",
        "            from spacy.pipeline.sentencizer import Sentencizer\r\n",
        "\r\n",
        "            nlp = spacy.load(\r\n",
        "                lang\r\n",
        "            )\r\n",
        "\r\n",
        "            # Add sentencizer\r\n",
        "            def make_sentencizer_config(repeats: int = 4, extra_punct_chars: Optional[List] = None) -> Dict[str, Any]:\r\n",
        "                punct_chars = []\r\n",
        "                for i in range(1, repeats + 1):\r\n",
        "                    for char in Sentencizer.default_punct_chars:\r\n",
        "                        punct_chars.append(char * i)\r\n",
        "\r\n",
        "                    if extra_punct_chars:\r\n",
        "                        for char in extra_punct_chars:\r\n",
        "                            punct_chars.append(char * i)\r\n",
        "\r\n",
        "                return {\"punct_chars\": punct_chars}\r\n",
        "            \r\n",
        "            sentencizer_config = make_sentencizer_config(repeats=4, extra_punct_chars=[\"\\n\"])\r\n",
        "            nlp.add_pipe(factory_name=\"sentencizer\", config=sentencizer_config, before=\"parser\")\r\n",
        "            cls._spacys[lang] = nlp\r\n",
        "        return cls._spacys[lang]\r\n",
        "\r\n",
        "class OrgRecognizer(EntityRecognizer):\r\n",
        "    \r\n",
        "    def load(self) -> None:\r\n",
        "        \"\"\"No loading is required.\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "    def analyze(self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts) -> List[RecognizerResult]:\r\n",
        "        \"\"\"\r\n",
        "        Analyzes test to find tokens which represent company names and organizations.\r\n",
        "        \"\"\"\r\n",
        "        results = []\r\n",
        "\r\n",
        "        current_ent = None\r\n",
        "        # iterate over the spaCy tokens, and call `token.ent_type_`\r\n",
        "        for token in nlp_artifacts.tokens:\r\n",
        "            if token.ent_type_ == \"ORG\":\r\n",
        "                if current_ent is None:\r\n",
        "                    current_ent = (token.idx, token.idx + len(token))\r\n",
        "                else:\r\n",
        "                    add = token.idx - current_ent[1]\r\n",
        "                    current_ent = (current_ent[0], current_ent[1] + len(token) + add)  # add whitespace\r\n",
        "            elif current_ent:\r\n",
        "                start, end = current_ent\r\n",
        "                result = RecognizerResult(\r\n",
        "                    entity_type=\"ORG\",\r\n",
        "                    start=start,\r\n",
        "                    end=end,\r\n",
        "                    score=0.99,\r\n",
        "                )\r\n",
        "                results.append(result)\r\n",
        "                current_ent = None\r\n",
        "\r\n",
        "        if current_ent:\r\n",
        "            start, end = current_ent\r\n",
        "            result = RecognizerResult(\r\n",
        "                entity_type=\"ORG\",\r\n",
        "                start=start,\r\n",
        "                end=end,\r\n",
        "                score=0.99,\r\n",
        "            )\r\n",
        "            results.append(result)\r\n",
        "        return results\r\n",
        "\r\n",
        "class AnalyzerMagic(object):\r\n",
        "    \"\"\"\r\n",
        "    Simple magic to minimize loading time for Presidio Analyzer.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _analyzer = None\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def get(cls, lang: str):\r\n",
        "        if not cls._analyzer:\r\n",
        "            import spacy\r\n",
        "            from presidio_analyzer import AnalyzerEngine\r\n",
        "            from presidio_analyzer.nlp_engine import SpacyNlpEngine\r\n",
        "            import tldextract\r\n",
        "\r\n",
        "            # Make sure that tldextract does not try to fetch data\r\n",
        "            no_fetch_extract = tldextract.TLDExtract(suffix_list_urls=())\r\n",
        "\r\n",
        "            # Define internal class inheriting from SpacyNlpEngine\r\n",
        "            class LoadedSpacyNlpEngine(SpacyNlpEngine):\r\n",
        "                def __init__(self, loaded_spacy_model):\r\n",
        "                    self.nlp = {\"en\": loaded_spacy_model}\r\n",
        "            \r\n",
        "            # Load spacy model\r\n",
        "            nlp = SpacyMagic.get(lang)\r\n",
        "            loaded_nlp_engine = LoadedSpacyNlpEngine(loaded_spacy_model=nlp)\r\n",
        "\r\n",
        "            # Create Presidio Analyzer\r\n",
        "            analyzer = AnalyzerEngine(\r\n",
        "                nlp_engine=loaded_nlp_engine\r\n",
        "            )\r\n",
        "\r\n",
        "            # Create org recognizer\r\n",
        "            org_recognizer = OrgRecognizer(supported_entities=[\"ORG\"])\r\n",
        "            analyzer.registry.add_recognizer(org_recognizer)\r\n",
        "\r\n",
        "            # Set Presidio Analyzer\r\n",
        "            cls._analyzer = analyzer\r\n",
        "        \r\n",
        "        # Return Presidio Analyzer\r\n",
        "        return cls._analyzer\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def library_config()-> None:\r\n",
        "        import os\r\n",
        "        import tldextract\r\n",
        "\r\n",
        "        # set timeout variable\r\n",
        "        os.environ[\"TLDEXTRACT_CACHE_TIMEOUT\"] = \"1.0\"\r\n",
        "\r\n",
        "        # Make sure that tldextract does not try to fetch data\r\n",
        "        tldextract.tldextract.TLD_EXTRACTOR.suffix_list_urls = ()\r\n",
        "        no_fetch_extract = tldextract.TLDExtract(suffix_list_urls=())\r\n",
        "\r\n",
        "class AnonymizerMagic(object):\r\n",
        "    \"\"\"\r\n",
        "    Simple magic to minimize loading time for Presidio Anonymizer.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    _anonymizer = None\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def get(cls):\r\n",
        "        if not cls._anonymizer:\r\n",
        "            from presidio_anonymizer import AnonymizerEngine\r\n",
        "\r\n",
        "            # Create Presidio Anonymizer\r\n",
        "            anonymizer = AnonymizerEngine()\r\n",
        "\r\n",
        "            # Set Presidio Anonymizer\r\n",
        "            cls._anonymizer = anonymizer\r\n",
        "        \r\n",
        "        # Return Presidio Anonymizer\r\n",
        "        return cls._anonymizer\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkm",
              "session_id": "17",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-04-20T08:19:05.8063538Z",
              "session_start_time": "2023-04-20T08:19:05.8661583Z",
              "execution_start_time": "2023-04-20T08:19:06.0055154Z",
              "execution_finish_time": "2023-04-20T08:20:39.7510289Z",
              "spark_jobs": null,
              "parent_msg_id": "7a391e90-ee52-4425-9422-f602c6d90fe5"
            },
            "text/plain": "StatementMeta(sparkm, 17, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": true,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the main anonymisation function. Customise as necessary...**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "import hashlib\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "\r\n",
        "from presidio_anonymizer.entities import OperatorConfig\r\n",
        "\r\n",
        "@F.udf(returnType=StringType())\r\n",
        "def anonymizeText(text):\r\n",
        "    # Check if text null or empty\r\n",
        "    if not text or len(text) <= 0:\r\n",
        "        return text\r\n",
        "\r\n",
        "    # Get Presidio Anonymizer and Analzer\r\n",
        "    AnalyzerMagic.library_config()\r\n",
        "    analyzer = AnalyzerMagic.get(\"en_core_web_lg\")\r\n",
        "    anonymizer = AnonymizerMagic.get()\r\n",
        "\r\n",
        "    # Analyze text\r\n",
        "    analyzer_results = analyzer.analyze(\r\n",
        "        text=text,\r\n",
        "        entities=[\r\n",
        "            \"CREDIT_CARD\",\r\n",
        "            \"CRYPTO\",\r\n",
        "            \"EMAIL_ADDRESS\",\r\n",
        "            \"IBAN_CODE\",\r\n",
        "            \"PERSON\",\r\n",
        "            \"PHONE_NUMBER\",\r\n",
        "            \"MEDICAL_LICENSE\",\r\n",
        "            \"URL\",\r\n",
        "            \"US_BANK_NUMBER\",\r\n",
        "            \"US_DRIVER_LICENSE\",\r\n",
        "            \"US_ITIN\",\r\n",
        "            \"US_PASSPORT\",\r\n",
        "            \"US_SSN\",\r\n",
        "            \"UK_NHS\",\r\n",
        "            \"NIF\",\r\n",
        "            \"FIN/NRIC\",\r\n",
        "            \"AU_ABN\",\r\n",
        "            \"AU_ACN\",\r\n",
        "            \"AU_TFN\",\r\n",
        "            \"AU_MEDICARE\",\r\n",
        "            \"ORG\"\r\n",
        "        ],\r\n",
        "        language=\"en\"\r\n",
        "    )\r\n",
        "\r\n",
        "    # Define mapping\r\n",
        "    mapping = {\r\n",
        "        \"CREDIT_CARD\": \"creditcard\",\r\n",
        "        \"CRYPTO\": \"crypto\",\r\n",
        "        \"EMAIL_ADDRESS\": \"email\",\r\n",
        "        \"IBAN_CODE\": \"iban\",\r\n",
        "        \"IP_ADDRESS\": \"ipaddress\",\r\n",
        "        \"LOCATION\": \"location\",\r\n",
        "        \"PERSON\": \"person\",\r\n",
        "        \"PHONE_NUMBER\": \"phone\",\r\n",
        "        \"MEDICAL_LICENSE\": \"medical\",\r\n",
        "        \"URL\": \"url\",\r\n",
        "        \"US_BANK_NUMBER\": \"usbank\",\r\n",
        "        \"US_DRIVER_LICENSE\": \"usdriver\",\r\n",
        "        \"US_ITIN\": \"usitin\",\r\n",
        "        \"US_PASSPORT\": \"uspassport\",\r\n",
        "        \"US_SSN\": \"usssn\",\r\n",
        "        \"UK_NHS\": \"uknhs\",\r\n",
        "        \"NIF\": \"nif\",\r\n",
        "        \"FIN/NRIC\": \"finnric\",\r\n",
        "        \"AU_ABN\": \"auabn\",\r\n",
        "        \"AU_ACN\": \"auacn\",\r\n",
        "        \"AU_TFN\": \"autfn\",\r\n",
        "        \"AU_MEDICARE\": \"usmedicare\",\r\n",
        "        \"DEFAULT\": \"other\",\r\n",
        "        \"ORG\": \"org\"\r\n",
        "    }\r\n",
        "\r\n",
        "    def get_placeholder(operator: str, item: str)-> str:\r\n",
        "        # Get mapping\r\n",
        "        placeholder_mapping = mapping[operator]\r\n",
        "\r\n",
        "        # Create hash\r\n",
        "        item_hash = hashlib.sha1(item.encode(\"UTF-8\")).hexdigest()\r\n",
        "        chars_hash = ''.join([i for i in item_hash if not i.isdigit()])\r\n",
        "        lower_hash = chars_hash.lower()+ chars_hash.lower()+ chars_hash.lower()\r\n",
        "        upper_hash = chars_hash.upper()+chars_hash.upper()+chars_hash.upper()\r\n",
        "        #Substitute characters in hash for all alpha characters. Note this will be different for every cell because the hash is always different\r\n",
        "        hashtable = str.maketrans(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\", lower_hash[:26]+upper_hash[:26])\r\n",
        "\r\n",
        "        return_hash =  item.translate(hashtable)\r\n",
        "        placeholder = f\"{return_hash}\"\r\n",
        "\r\n",
        "        return placeholder\r\n",
        "\r\n",
        "\r\n",
        "    # Anonymize Text\r\n",
        "    try:\r\n",
        "        anonymizer_result = anonymizer.anonymize(\r\n",
        "            text=text,\r\n",
        "            analyzer_results=[RecognizerResult('DEFAULT', 0, len(text), 0.85)],#analyzer_results,\r\n",
        "            operators={\r\n",
        "                \"CREDIT_CARD\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"CREDIT_CARD\", x)}),\r\n",
        "                \"CRYPTO\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"CRYPTO\", x)}),\r\n",
        "                \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"EMAIL_ADDRESS\", x)}),\r\n",
        "                \"IBAN_CODE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"IBAN_CODE\", x)}),\r\n",
        "                \"IP_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"IP_ADDRESS\", x)}),\r\n",
        "                \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"LOCATION\", x)}),\r\n",
        "                \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"PERSON\", x)}),\r\n",
        "                \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"PHONE_NUMBER\", x)}),\r\n",
        "                \"MEDICAL_LICENSE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"MEDICAL_LICENSE\", x)}),\r\n",
        "                \"URL\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"URL\", x)}),\r\n",
        "                \"US_BANK_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"US_BANK_NUMBER\", x)}),\r\n",
        "                \"US_DRIVER_LICENSE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"US_DRIVER_LICENSE\", x)}),\r\n",
        "                \"US_ITIN\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"US_ITIN\", x)}),\r\n",
        "                \"US_PASSPORT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"US_PASSPORT\", x)}),\r\n",
        "                \"US_SSN\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"US_SSN\", x)}),\r\n",
        "                \"UK_NHS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"UK_NHS\", x)}),\r\n",
        "                \"NIF\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"NIF\", x)}),\r\n",
        "                \"FIN/NRIC\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"FIN/NRIC\", x)}),\r\n",
        "                \"AU_ABN\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"AU_ABN\", x)}),\r\n",
        "                \"AU_ACN\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"AU_ACN\", x)}),\r\n",
        "                \"AU_TFN\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"AU_TFN\", x)}),\r\n",
        "                \"AU_MEDICARE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"AU_MEDICARE\", x)}),\r\n",
        "                \"DEFAULT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"DEFAULT\", x)}),\r\n",
        "                \"ORG\": OperatorConfig(\"custom\", {\"lambda\": lambda x: get_placeholder(\"ORG\", x)})\r\n",
        "            },\r\n",
        "        )\r\n",
        "        return anonymizer_result.text\r\n",
        "    except:\r\n",
        "        return \"Exception\"\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkm",
              "session_id": "17",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-04-20T08:19:05.8734984Z",
              "session_start_time": null,
              "execution_start_time": "2023-04-20T08:20:39.9313628Z",
              "execution_finish_time": "2023-04-20T08:20:40.5086096Z",
              "spark_jobs": null,
              "parent_msg_id": "1adc3bc7-2f95-4a83-92dd-5868a584a799"
            },
            "text/plain": "StatementMeta(sparkm, 17, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": true,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create sample data and show output before running the anonymisation routine...**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\r\n",
        "    (\"Michael\",\"John\",\"Riley\",\"40288\",\"M\",4000),\r\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\r\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\r\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\r\n",
        "  ]\r\n",
        "\r\n",
        "schema = StructType([ \\\r\n",
        "    StructField(\"firstname\",StringType(),True), \\\r\n",
        "    StructField(\"middlename\",StringType(),True), \\\r\n",
        "    StructField(\"lastname\",StringType(),True), \\\r\n",
        "    StructField(\"id\", StringType(), True), \\\r\n",
        "    StructField(\"gender\", StringType(), True), \\\r\n",
        "    StructField(\"salary\", IntegerType(), True) \\\r\n",
        "  ])\r\n",
        " \r\n",
        "df = spark.createDataFrame(data=data2,schema=schema)\r\n",
        "#df.printSchema()\r\n",
        "#df.show(truncate=False)\r\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkm",
              "session_id": "17",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-04-20T08:26:11.9152195Z",
              "session_start_time": null,
              "execution_start_time": "2023-04-20T08:26:12.0652377Z",
              "execution_finish_time": "2023-04-20T08:26:13.162741Z",
              "spark_jobs": null,
              "parent_msg_id": "8f6ef4ce-6d3d-49e4-9a61-7da2e5da6b64"
            },
            "text/plain": "StatementMeta(sparkm, 17, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "0a414ff9-c952-4319-bee8-46cf79b41d44",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 0a414ff9-c952-4319-bee8-46cf79b41d44)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now specify the columns to be anonymised and run the anonymisation routine for each of the columns...**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnstoanonymize = ['firstname','lastname']\r\n",
        "for col_name in df.columns:\r\n",
        "    for columntoanonymize in columnstoanonymize:\r\n",
        "       \r\n",
        "        if col_name == columntoanonymize:\r\n",
        "            df = df.withColumn(\r\n",
        "                col_name, anonymizeText(F.col(col_name)))\r\n",
        "\r\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkm",
              "session_id": "17",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-04-20T08:27:01.4884948Z",
              "session_start_time": null,
              "execution_start_time": "2023-04-20T08:27:02.113943Z",
              "execution_finish_time": "2023-04-20T08:28:40.9877632Z",
              "spark_jobs": null,
              "parent_msg_id": "e7131e60-2f7a-4436-9346-687a32918f03"
            },
            "text/plain": "StatementMeta(sparkm, 17, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "6a145481-808b-4afc-ae84-df166cd9c79b",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 6a145481-808b-4afc-ae84-df166cd9c79b)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "0a414ff9-c952-4319-bee8-46cf79b41d44": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "James",
                  "1": "",
                  "2": "Smith",
                  "3": "36636",
                  "4": "M",
                  "5": "3000"
                },
                {
                  "0": "Michael",
                  "1": "Rose",
                  "2": "",
                  "3": "40288",
                  "4": "M",
                  "5": "4000"
                },
                {
                  "0": "Robert",
                  "1": "",
                  "2": "Williams",
                  "3": "42114",
                  "4": "M",
                  "5": "4000"
                },
                {
                  "0": "Maria",
                  "1": "Anne",
                  "2": "Jones",
                  "3": "39192",
                  "4": "F",
                  "5": "4000"
                },
                {
                  "0": "Jen",
                  "1": "Mary",
                  "2": "Brown",
                  "3": "",
                  "4": "F",
                  "5": "-1"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "firstname",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "middlename",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "lastname",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "id",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "gender",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "salary",
                  "type": "int"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "5"
                ],
                "isStacked": false
              }
            }
          }
        },
        "6a145481-808b-4afc-ae84-df166cd9c79b": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "Ceddd",
                  "1": "",
                  "2": "Cfefc",
                  "3": "36636",
                  "4": "M",
                  "5": "3000"
                },
                {
                  "0": "Bebcfad",
                  "1": "Rose",
                  "2": "",
                  "3": "40288",
                  "4": "M",
                  "5": "4000"
                },
                {
                  "0": "Bdfdbc",
                  "1": "",
                  "2": "Caeeacca",
                  "3": "42114",
                  "4": "M",
                  "5": "4000"
                },
                {
                  "0": "Eedee",
                  "1": "Anne",
                  "2": "Cadfb",
                  "3": "39192",
                  "4": "F",
                  "5": "4000"
                },
                {
                  "0": "Dca",
                  "1": "Mary",
                  "2": "Bacaa",
                  "3": "",
                  "4": "F",
                  "5": "-1"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "firstname",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "middlename",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "lastname",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "id",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "gender",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "salary",
                  "type": "int"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "5"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}