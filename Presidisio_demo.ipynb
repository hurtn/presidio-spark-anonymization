{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\r\n",
        "from presidio_anonymizer import AnonymizerEngine\r\n",
        "from presidio_anonymizer.entities import OperatorConfig,EngineResult\r\n",
        "from pyspark.sql.functions import col, pandas_udf\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "\r\n",
        "#read the csv file into dataframe\r\n",
        "df1 = spark.read.load('abfss://sensitive@pvdemoar4sisynapsedl.dfs.core.windows.net/customers/customers.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "#display(df1.limit(10))\r\n",
        "\r\n",
        "#take a sample for detection/analysis\r\n",
        "detectionsample = 10\r\n",
        "dfsample= df1.limit(detectionsample).toPandas()\r\n",
        "\r\n",
        "# DataFrame to dict\r\n",
        "df_dict = dfsample.to_dict(orient=\"list\")\r\n",
        "\r\n",
        "#initialise the analyzer engine and analyze the sample for PII\r\n",
        "analyzer = AnalyzerEngine()\r\n",
        "batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\r\n",
        "#batch_anonymizer = BatchAnonymizerEngine()\r\n",
        "analyzer_results = batch_analyzer.analyze_dict(df_dict, language=\"en\")\r\n",
        "analyzer_results = list(analyzer_results)\r\n",
        "\r\n",
        "#debugging line to print out results\r\n",
        "#analyzer_results\r\n",
        "#print(analyzer_results[3].key+':'+str(analyzer_results[3].recognizer_results[3][0]))\r\n",
        "columntoanonymize = analyzer_results[3].key\r\n",
        "\r\n",
        "anonymizer = AnonymizerEngine()\r\n",
        "\r\n",
        "# broadcast the engines to the cluster nodes\r\n",
        "broadcasted_analyzer = sc.broadcast(analyzer)\r\n",
        "broadcasted_anonymizer = sc.broadcast(anonymizer)\r\n",
        "\r\n",
        "# define a pandas UDF function and a series function over it.\r\n",
        "def anonymize_text(text: str) -> str:\r\n",
        "    analyzer = broadcasted_analyzer.value\r\n",
        "    anonymizer = broadcasted_anonymizer.value\r\n",
        "    analyzer_results = analyzer.analyze(text=text, language=\"en\")\r\n",
        "    anonymized_results = anonymizer.anonymize(\r\n",
        "        text=text,\r\n",
        "        analyzer_results=analyzer_results,\r\n",
        "        operators={\r\n",
        "            \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})\r\n",
        "        },\r\n",
        "    )\r\n",
        "    return anonymized_results.text\r\n",
        "\r\n",
        "\r\n",
        "def anonymize_series(s: pd.Series) -> pd.Series:\r\n",
        "    return s.apply(anonymize_text)\r\n",
        "\r\n",
        "\r\n",
        "# define a the function as pandas UDF\r\n",
        "anonymize = pandas_udf(anonymize_series, returnType=StringType())\r\n",
        "\r\n",
        "# apply the udf\r\n",
        "anonymized_df = df1.withColumn(\r\n",
        "    columntoanonymize, anonymize(col(columntoanonymize))\r\n",
        ")\r\n",
        "display(anonymized_df)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}